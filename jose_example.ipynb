{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.recommendation import ALS as lib_ALS\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark = ps.sql.SparkSession.builder \\\n",
    "            .master(\"local[4]\") \\\n",
    "            .appName(\"building recommender\") \\\n",
    "            .getOrCreate()\n",
    "            \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subset_raw_data = sc.textFile('data/subset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u',id,source,reader_id,from_book_id,book_id,ad_id,boost_id,clicked,claimed,optin,created_at,updated_at',\n",
       " u'29295882,29295919,success_page,186643,20370,19203,0,0,1,1,0,2017-02-20 02:16:11,2017-02-20 02:26:24',\n",
       " u'29295883,29295920,success_page,186643,20370,19813,0,0,0,0,0,2017-02-20 02:16:11,2017-02-20 02:16:11']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print type(subset_raw_data)\n",
    "subset_raw_data.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u',id,source,reader_id,from_book_id,book_id,ad_id,boost_id,clicked,claimed,optin,created_at,updated_at'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_raw_data_header = subset_raw_data.take(1)[0]\n",
    "subset_raw_data_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clicked_not_claimed_to_neg1(claimed, clicked):\n",
    "    if (clicked == 1) and (claimed == 0):\n",
    "        return -1\n",
    "    else:\n",
    "        return claimed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subset_raw_data_test_schema = subset_raw_data.filter(lambda line: line!=subset_raw_data_header).map(lambda line: line.split(\",\")).map(lambda p: Row(reader_id=p[3], book_id=p[5], clicked=int(p[8]), claimed=int(p[9])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subset_data_test_df = spark.createDataFrame(subset_raw_data_test_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+---------+\n",
      "|book_id|claimed|clicked|reader_id|\n",
      "+-------+-------+-------+---------+\n",
      "|  19203|      1|      1|   186643|\n",
      "|  19813|      0|      0|   186643|\n",
      "|  16281|      0|      0|   523754|\n",
      "|  22669|      1|      1|   523754|\n",
      "|  25350|      0|      0|   523754|\n",
      "|  17251|      0|      0|    17224|\n",
      "|  19012|      0|      0|    17224|\n",
      "|  20771|      1|      1|    17224|\n",
      "|   7544|      0|      0|   239648|\n",
      "|   8682|      0|      0|   239648|\n",
      "|  26798|      0|      0|   239648|\n",
      "|  22094|      0|      1|    13391|\n",
      "|  23867|      0|      0|    13391|\n",
      "|  26736|      1|      1|    13391|\n",
      "|   4132|      0|      0|    31393|\n",
      "+-------+-------+-------+---------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subset_data_test_df.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_udf = udf(lambda claimed, clicked : clicked_not_claimed_to_neg1(claimed, clicked), IntegerType())\n",
    "df_out = subset_data_test_df.withColumn('claimed', my_udf(col('claimed'), col('clicked')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+---------+\n",
      "|book_id|claimed|clicked|reader_id|\n",
      "+-------+-------+-------+---------+\n",
      "|  19203|      1|      1|   186643|\n",
      "|  19813|      0|      0|   186643|\n",
      "|  16281|      0|      0|   523754|\n",
      "|  22669|      1|      1|   523754|\n",
      "|  25350|      0|      0|   523754|\n",
      "|  17251|      0|      0|    17224|\n",
      "|  19012|      0|      0|    17224|\n",
      "|  20771|      1|      1|    17224|\n",
      "|   7544|      0|      0|   239648|\n",
      "|   8682|      0|      0|   239648|\n",
      "|  26798|      0|      0|   239648|\n",
      "|  22094|     -1|      1|    13391|\n",
      "|  23867|      0|      0|    13391|\n",
      "|  26736|      1|      1|    13391|\n",
      "|   4132|      0|      0|    31393|\n",
      "+-------+-------+-------+---------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_out.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subset_test_rdd_raw = df_out.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(book_id=u'19203', claimed=1, clicked=1, reader_id=u'186643'),\n",
       " Row(book_id=u'19813', claimed=0, clicked=0, reader_id=u'186643'),\n",
       " Row(book_id=u'16281', claimed=0, clicked=0, reader_id=u'523754'),\n",
       " Row(book_id=u'22669', claimed=1, clicked=1, reader_id=u'523754'),\n",
       " Row(book_id=u'25350', claimed=0, clicked=0, reader_id=u'523754'),\n",
       " Row(book_id=u'17251', claimed=0, clicked=0, reader_id=u'17224'),\n",
       " Row(book_id=u'19012', claimed=0, clicked=0, reader_id=u'17224'),\n",
       " Row(book_id=u'20771', claimed=1, clicked=1, reader_id=u'17224'),\n",
       " Row(book_id=u'7544', claimed=0, clicked=0, reader_id=u'239648'),\n",
       " Row(book_id=u'8682', claimed=0, clicked=0, reader_id=u'239648'),\n",
       " Row(book_id=u'26798', claimed=0, clicked=0, reader_id=u'239648'),\n",
       " Row(book_id=u'22094', claimed=-1, clicked=1, reader_id=u'13391'),\n",
       " Row(book_id=u'23867', claimed=0, clicked=0, reader_id=u'13391'),\n",
       " Row(book_id=u'26736', claimed=1, clicked=1, reader_id=u'13391'),\n",
       " Row(book_id=u'4132', claimed=0, clicked=0, reader_id=u'31393')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_test_rdd_raw.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subset_data_test = subset_test_rdd_raw.map(lambda tokens: (tokens[3],tokens[0],tokens[1])).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'186643', u'19203', 1),\n",
       " (u'186643', u'19813', 0),\n",
       " (u'523754', u'16281', 0),\n",
       " (u'523754', u'22669', 1),\n",
       " (u'523754', u'25350', 0),\n",
       " (u'17224', u'17251', 0),\n",
       " (u'17224', u'19012', 0),\n",
       " (u'17224', u'20771', 1),\n",
       " (u'239648', u'7544', 0),\n",
       " (u'239648', u'8682', 0),\n",
       " (u'239648', u'26798', 0),\n",
       " (u'13391', u'22094', -1),\n",
       " (u'13391', u'23867', 0),\n",
       " (u'13391', u'26736', 1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_data_test.take(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subset_data = subset_raw_data.filter(lambda line: line!=subset_raw_data_header)\\\n",
    "    .map(lambda line: line.split(\",\")).map(lambda tokens: (tokens[3],tokens[5],tokens[9])).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'186643', u'19203', u'1'),\n",
       " (u'186643', u'19813', u'0'),\n",
       " (u'523754', u'16281', u'0'),\n",
       " (u'523754', u'22669', u'1')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_data.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_RDD, validation_RDD, test_RDD = subset_data_test.randomSplit([6, 2, 2], seed=0L)\n",
    "validation_for_predict_RDD = validation_RDD.map(lambda x: (x[0], x[1]))\n",
    "test_for_predict_RDD = test_RDD.map(lambda x: (x[0], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed = 5L\n",
    "iterations = 20\n",
    "regularization_parameter = 0.05\n",
    "ranks = [10, 15, 20, 25, 30]\n",
    "rank = 25\n",
    "errors = [0, 0, 0, 0, 0]\n",
    "reg_met = [0, 0, 0, 0, 0]\n",
    "err = 0\n",
    "tolerance = 0.02\n",
    "\n",
    "# min_error = float('inf')\n",
    "# best_rank = -1\n",
    "# best_iteration = -1\n",
    "# for rank in ranks:\n",
    "#     model = lib_ALS.train(training_RDD, rank=rank, seed=seed, iterations=iterations,\\\n",
    "#                       lambda_=regularization_parameter)\n",
    "# #     model = lib_ALS.trainImplicit(training_RDD, rank=rank, seed=seed, iterations=iterations,\\\n",
    "# #                               lambda_=regularization_parameter)\n",
    "#     predictions = model.predictAll(validation_for_predict_RDD)\\\n",
    "#                         .map(lambda r: ((r[0], r[1]), r[2]))\n",
    "\n",
    "#     rates_and_preds = validation_RDD\\\n",
    "#                         .map(lambda r: ((int(r[0]), int(r[1])), float(r[2])))\\\n",
    "#                         .join(predictions)\n",
    "#     valuesAndPreds = rates_and_preds\\\n",
    "#                         .map(lambda p: (p[1]))\n",
    "#     metrics = RegressionMetrics(valuesAndPreds)\n",
    "#     error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "#     errors[err] = error\n",
    "#     reg_met[err] = metrics.rootMeanSquaredError\n",
    "#     err += 1\n",
    "#     print 'For rank %s the RMSE is %s the reg_met is %s' % (rank, error,\\\n",
    "#                                                             metrics.rootMeanSquaredError)\n",
    "#     if error < min_error:\n",
    "#         min_error = error\n",
    "#         best_rank = rank\n",
    "\n",
    "# print 'The best model was trained with rank %s' % best_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = lib_ALS.train(training_RDD, rank=rank, seed=seed, iterations=iterations,\\\n",
    "                      lambda_=regularization_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predictAll(validation_for_predict_RDD)\\\n",
    "                        .map(lambda r: ((r[0], r[1]), r[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rates_and_preds = validation_RDD\\\n",
    "                        .map(lambda r: ((int(r[0]), int(r[1])), float(r[2])))\\\n",
    "                        .join(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valuesAndPreds = rates_and_preds.map(lambda p: (p[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics = RegressionMetrics(valuesAndPreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job 19 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:808)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:806)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:806)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1668)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:83)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1587)\n\tat org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1826)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1283)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1825)\n\tat org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:581)\n\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:934)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-017468885028>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrates_and_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/davidclausen/spark-2.1.0-bin-hadoop2.7/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mmean\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[0;36m2.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \"\"\"\n\u001b[0;32m-> 1188\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvariance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davidclausen/spark-2.1.0-bin-hadoop2.7/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mstats\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mleft_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmergeStats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_counter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStatCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mredFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuckets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davidclausen/spark-2.1.0-bin-hadoop2.7/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    833\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davidclausen/spark-2.1.0-bin-hadoop2.7/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m         \"\"\"\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davidclausen/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davidclausen/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/davidclausen/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job 19 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:808)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:806)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:806)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1668)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:83)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1587)\n\tat org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1826)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1283)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1825)\n\tat org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:581)\n\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:934)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 62658)\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/davidclausen/anaconda2/lib/python2.7/SocketServer.py\", line 290, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/Users/davidclausen/anaconda2/lib/python2.7/SocketServer.py\", line 318, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/Users/davidclausen/anaconda2/lib/python2.7/SocketServer.py\", line 331, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/Users/davidclausen/anaconda2/lib/python2.7/SocketServer.py\", line 652, in __init__\n",
      "    self.handle()\n",
      "  File \"/Users/davidclausen/spark-2.1.0-bin-hadoop2.7/python/pyspark/accumulators.py\", line 235, in handle\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/Users/davidclausen/spark-2.1.0-bin-hadoop2.7/python/pyspark/serializers.py\", line 557, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    }
   ],
   "source": [
    "error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "errors[err] = error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg_met[err] = metrics.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "err += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rank 25 the RMSE is 0.293608735816 the reg_met is 0.293608735816\n"
     ]
    }
   ],
   "source": [
    "print 'For rank %s the RMSE is %s the reg_met is %s' % (rank, error,\\\n",
    "                                                            metrics.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if error < min_error:\n",
    "    min_error = error\n",
    "    best_rank = rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model was trained with rank 25\n"
     ]
    }
   ],
   "source": [
    "print 'The best model was trained with rank %s' % best_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unicode"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(subset_data.take(1)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((223813, 20831), (0.0, 0.0899249391151878)),\n",
       " ((163115, 19627), (0.0, 0.09408654156834773)),\n",
       " ((369590, 19130), (0.0, -0.05952669047578264)),\n",
       " ((528665, 21987), (0.0, 0.0)),\n",
       " ((86810, 20764), (0.0, 0.24387854400033937)),\n",
       " ((136815, 12427), (0.0, 0.0)),\n",
       " ((416521, 24939), (1.0, 0.19363965726346982)),\n",
       " ((385023, 18231), (0.0, 0.0)),\n",
       " ((567643, 24095), (0.0, 0.0)),\n",
       " ((538803, 19499), (1.0, 0.7973047413843842)),\n",
       " ((304226, 24796), (0.0, 0.04156643639615049)),\n",
       " ((81070, 15836), (1.0, 0.13718560371824387)),\n",
       " ((236093, 12543), (0.0, 0.0)),\n",
       " ((418280, 23924), (0.0, 0.0)),\n",
       " ((349699, 26067), (1.0, 0.0)),\n",
       " ((481977, 7663), (0.0, 0.36049119541868013)),\n",
       " ((12732, 20796), (1.0, 0.10949134786201241)),\n",
       " ((129149, 25501), (1.0, 0.1527517510284398)),\n",
       " ((300481, 22695), (1.0, 0.42263723221974947)),\n",
       " ((166480, 18534), (0.0, 0.0)),\n",
       " ((444559, 11395), (0.0, 0.0)),\n",
       " ((64813, 15493), (0.0, 0.0)),\n",
       " ((508958, 10634), (0.0, 0.0)),\n",
       " ((508958, 10634), (0.0, 0.0)),\n",
       " ((508958, 10634), (0.0, 0.0)),\n",
       " ((508958, 10634), (0.0, 0.0)),\n",
       " ((568768, 10634), (0.0, 0.0)),\n",
       " ((76076, 18160), (0.0, 0.0)),\n",
       " ((466749, 13807), (0.0, -0.07471389896023696)),\n",
       " ((246848, 25632), (1.0, 0.027699399031606075)),\n",
       " ((205230, 6896), (0.0, 0.01751887100087232)),\n",
       " ((440648, 25430), (0.0, 0.3488141753956898)),\n",
       " ((560531, 16773), (1.0, 0.25045933167163703)),\n",
       " ((400262, 19732), (0.0, 0.0)),\n",
       " ((139379, 20793), (0.0, 0.0030218969074367642)),\n",
       " ((293639, 22011), (0.0, 0.28766755462878135)),\n",
       " ((107779, 22823), (0.0, 0.0)),\n",
       " ((539520, 14268), (0.0, 0.1313993435527248)),\n",
       " ((532434, 25816), (0.0, 0.0973992144002585)),\n",
       " ((296824, 17456), (0.0, 0.0)),\n",
       " ((401345, 14329), (0.0, 0.0)),\n",
       " ((506577, 21341), (0.0, 0.014581390748003303)),\n",
       " ((352542, 24418), (0.0, 0.2712497194110065)),\n",
       " ((389088, 24752), (0.0, -0.045606826819271806)),\n",
       " ((430325, 19725), (0.0, 0.0)),\n",
       " ((224082, 15404), (0.0, 0.0)),\n",
       " ((74720, 19200), (0.0, 0.0)),\n",
       " ((346893, 20805), (0.0, 0.0)),\n",
       " ((142649, 22087), (0.0, 0.0)),\n",
       " ((317417, 24413), (0.0, -0.020286525921586404)),\n",
       " ((521781, 19199), (0.0, 0.0)),\n",
       " ((567316, 19730), (0.0, 0.0)),\n",
       " ((342715, 24957), (0.0, 0.0)),\n",
       " ((376785, 21679), (0.0, 0.0)),\n",
       " ((12887, 26291), (0.0, 0.07452559951511611)),\n",
       " ((227814, 17970), (0.0, 0.07182770805631883)),\n",
       " ((182722, 14994), (0.0, -0.011751438307164054)),\n",
       " ((133918, 16290), (0.0, -0.0701190648482819)),\n",
       " ((555931, 19731), (0.0, 0.0)),\n",
       " ((295493, 19731), (0.0, 0.0)),\n",
       " ((466319, 17387), (0.0, -0.0355716009549976)),\n",
       " ((201864, 14322), (0.0, 0.1335760815752986)),\n",
       " ((345654, 17894), (0.0, 0.0)),\n",
       " ((337630, 20990), (0.0, 0.0)),\n",
       " ((567201, 8363), (0.0, 0.0)),\n",
       " ((244393, 15481), (0.0, 0.025794955750060272)),\n",
       " ((566913, 26659), (0.0, 0.0)),\n",
       " ((369847, 22519), (0.0, 0.22604060644677962)),\n",
       " ((339390, 18530), (0.0, 0.0)),\n",
       " ((440465, 21975), (0.0, 0.0)),\n",
       " ((255382, 25564), (0.0, 0.25139617004776926)),\n",
       " ((566986, 14402), (0.0, 0.0)),\n",
       " ((61998, 20266), (0.0, 0.15615868370598382)),\n",
       " ((561179, 23055), (0.0, 0.0)),\n",
       " ((21003, 25251), (0.0, 0.0)),\n",
       " ((136729, 25325), (0.0, 0.0)),\n",
       " ((34156, 6852), (0.0, 0.0)),\n",
       " ((71858, 12562), (0.0, 0.0)),\n",
       " ((292955, 24095), (0.0, 0.0)),\n",
       " ((552259, 10605), (0.0, 0.00367779570364735)),\n",
       " ((465406, 16020), (1.0, -0.06648495742738142)),\n",
       " ((474551, 13265), (0.0, 0.009007141255525902)),\n",
       " ((168555, 24483), (0.0, 0.17817897868724497)),\n",
       " ((208935, 25625), (1.0, -0.04443189404283537)),\n",
       " ((430018, 21954), (0.0, 0.0)),\n",
       " ((487680, 21084), (0.0, 0.0992580687186742)),\n",
       " ((1011, 15819), (0.0, 0.0)),\n",
       " ((50109, 13467), (0.0, 0.06256486760908583)),\n",
       " ((26434, 23888), (0.0, -0.0349000817219146)),\n",
       " ((217248, 26842), (0.0, 0.15118843938684792)),\n",
       " ((478622, 17826), (0.0, 0.14725561306433932)),\n",
       " ((417441, 23857), (0.0, 0.0)),\n",
       " ((567290, 18492), (0.0, 0.0)),\n",
       " ((329587, 11267), (1.0, 0.027655230538519086)),\n",
       " ((335180, 12370), (0.0, 0.0)),\n",
       " ((215543, 19029), (0.0, 0.0)),\n",
       " ((150201, 16227), (0.0, 0.06735543505678768)),\n",
       " ((179732, 22194), (0.0, 0.0)),\n",
       " ((84233, 5217), (0.0, -0.04610271666704602)),\n",
       " ((491101, 23861), (1.0, -0.12254628098057838))]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rates_and_preds.take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((90628, 17981), 0.0),\n",
       " ((567356, 7550), 0.0),\n",
       " ((182316, 19732), 0.0),\n",
       " ((182316, 19730), 0.0),\n",
       " ((567788, 8363), 0.0)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 0.046730506967385704),\n",
       " (0.0, 0.18970916262654078),\n",
       " (0.0, 0.011221973957054456),\n",
       " (0.0, 0.0),\n",
       " (0.0, -0.00655216909141193)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valuesAndPreds.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
